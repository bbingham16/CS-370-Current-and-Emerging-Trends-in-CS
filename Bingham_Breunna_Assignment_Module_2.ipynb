{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 17s 349us/step - loss: 1.4829 - accuracy: 0.6231 - val_loss: 0.7584 - val_accuracy: 0.8286\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 20s 408us/step - loss: 0.6049 - accuracy: 0.8464 - val_loss: 0.4550 - val_accuracy: 0.8852\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 18s 384us/step - loss: 0.4398 - accuracy: 0.8801 - val_loss: 0.3710 - val_accuracy: 0.9019\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 14s 300us/step - loss: 0.3767 - accuracy: 0.8952 - val_loss: 0.3322 - val_accuracy: 0.9082\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 95us/step - loss: 0.3415 - accuracy: 0.9025 - val_loss: 0.3055 - val_accuracy: 0.9147\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.3175 - accuracy: 0.9086 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2989 - accuracy: 0.9137 - val_loss: 0.2727 - val_accuracy: 0.9224\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 86us/step - loss: 0.2839 - accuracy: 0.9180 - val_loss: 0.2608 - val_accuracy: 0.9266\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.2714 - accuracy: 0.9217 - val_loss: 0.2505 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2602 - accuracy: 0.9252 - val_loss: 0.2430 - val_accuracy: 0.9308\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.2501 - accuracy: 0.9285 - val_loss: 0.2341 - val_accuracy: 0.9335\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2409 - accuracy: 0.9301 - val_loss: 0.2271 - val_accuracy: 0.9352\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.2325 - accuracy: 0.9334 - val_loss: 0.2227 - val_accuracy: 0.9367\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.2253 - accuracy: 0.9353 - val_loss: 0.2147 - val_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2181 - accuracy: 0.9375 - val_loss: 0.2082 - val_accuracy: 0.9411\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2116 - accuracy: 0.9394 - val_loss: 0.2030 - val_accuracy: 0.9431\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 96us/step - loss: 0.2055 - accuracy: 0.9414 - val_loss: 0.1981 - val_accuracy: 0.9445\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.1996 - accuracy: 0.9430 - val_loss: 0.1932 - val_accuracy: 0.9458\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1941 - accuracy: 0.9432 - val_loss: 0.1894 - val_accuracy: 0.9467\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.1890 - accuracy: 0.9456 - val_loss: 0.1849 - val_accuracy: 0.9498\n",
      "10000/10000 [==============================] - 0s 42us/step\n",
      "Test score: 0.18599770209044217\n",
      "Test accuracy: 0.9463000297546387\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128  \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved  for Validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 103us/step - loss: 1.5325 - accuracy: 0.6104 - val_loss: 0.6978 - val_accuracy: 0.8483\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 100us/step - loss: 0.5411 - accuracy: 0.8575 - val_loss: 0.4142 - val_accuracy: 0.8887\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.3998 - accuracy: 0.8863 - val_loss: 0.3474 - val_accuracy: 0.9021\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.3478 - accuracy: 0.9001 - val_loss: 0.3126 - val_accuracy: 0.9100\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 97us/step - loss: 0.3164 - accuracy: 0.9082 - val_loss: 0.2888 - val_accuracy: 0.9172\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 10s 201us/step - loss: 0.2930 - accuracy: 0.9149 - val_loss: 0.2694 - val_accuracy: 0.9221\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 6s 129us/step - loss: 0.2740 - accuracy: 0.9205 - val_loss: 0.2556 - val_accuracy: 0.9272\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.2573 - accuracy: 0.9255 - val_loss: 0.2414 - val_accuracy: 0.9292\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 8s 167us/step - loss: 0.2435 - accuracy: 0.9290 - val_loss: 0.2314 - val_accuracy: 0.9320\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 9s 189us/step - loss: 0.2306 - accuracy: 0.9325 - val_loss: 0.2217 - val_accuracy: 0.9356\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 10s 202us/step - loss: 0.2193 - accuracy: 0.9356 - val_loss: 0.2112 - val_accuracy: 0.9382\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 7s 144us/step - loss: 0.2087 - accuracy: 0.9393 - val_loss: 0.2053 - val_accuracy: 0.9413\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 6s 118us/step - loss: 0.1992 - accuracy: 0.9418 - val_loss: 0.1952 - val_accuracy: 0.9441\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1906 - accuracy: 0.9447 - val_loss: 0.1907 - val_accuracy: 0.9462\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.1822 - accuracy: 0.9467 - val_loss: 0.1851 - val_accuracy: 0.9475\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.1751 - accuracy: 0.9491 - val_loss: 0.1784 - val_accuracy: 0.9495\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 3s 63us/step - loss: 0.1679 - accuracy: 0.9510 - val_loss: 0.1704 - val_accuracy: 0.9525\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.1615 - accuracy: 0.9527 - val_loss: 0.1663 - val_accuracy: 0.9534\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1553 - accuracy: 0.9551 - val_loss: 0.1626 - val_accuracy: 0.9545\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 64us/step - loss: 0.1499 - accuracy: 0.9566 - val_loss: 0.1569 - val_accuracy: 0.9574\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "Test score: 0.1540503367677331\n",
      "Test accuracy: 0.9546999931335449\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128  \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved  for Validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 167,818\n",
      "Trainable params: 167,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 2.0505 - accuracy: 0.3756 - val_loss: 1.2873 - val_accuracy: 0.6512\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.7604 - accuracy: 0.7886 - val_loss: 0.4856 - val_accuracy: 0.8628\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.4377 - accuracy: 0.8724 - val_loss: 0.3596 - val_accuracy: 0.8959\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.3536 - accuracy: 0.8976 - val_loss: 0.3051 - val_accuracy: 0.9112\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 6s 121us/step - loss: 0.3078 - accuracy: 0.9101 - val_loss: 0.2731 - val_accuracy: 0.9218\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 6s 115us/step - loss: 0.2757 - accuracy: 0.9201 - val_loss: 0.2460 - val_accuracy: 0.9310\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 90us/step - loss: 0.2501 - accuracy: 0.9259 - val_loss: 0.2315 - val_accuracy: 0.9318\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.2299 - accuracy: 0.9325 - val_loss: 0.2214 - val_accuracy: 0.9349\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.2129 - accuracy: 0.9371 - val_loss: 0.2100 - val_accuracy: 0.9382\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.1984 - accuracy: 0.9417 - val_loss: 0.1927 - val_accuracy: 0.9441\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1863 - accuracy: 0.9446 - val_loss: 0.1788 - val_accuracy: 0.9476\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1746 - accuracy: 0.9482 - val_loss: 0.1719 - val_accuracy: 0.9498\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1647 - accuracy: 0.9514 - val_loss: 0.1627 - val_accuracy: 0.9521\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.1558 - accuracy: 0.9536 - val_loss: 0.1563 - val_accuracy: 0.9540\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.1470 - accuracy: 0.9558 - val_loss: 0.1526 - val_accuracy: 0.9566\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 89us/step - loss: 0.1400 - accuracy: 0.9586 - val_loss: 0.1489 - val_accuracy: 0.9555\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 124us/step - loss: 0.1330 - accuracy: 0.9610 - val_loss: 0.1554 - val_accuracy: 0.9528\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.1271 - accuracy: 0.9626 - val_loss: 0.1440 - val_accuracy: 0.9584\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 6s 133us/step - loss: 0.1211 - accuracy: 0.9645 - val_loss: 0.1404 - val_accuracy: 0.9574\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 7s 140us/step - loss: 0.1162 - accuracy: 0.9659 - val_loss: 0.1323 - val_accuracy: 0.9597\n",
      "10000/10000 [==============================] - 0s 48us/step\n",
      "Test score: 0.13334855514466762\n",
      "Test accuracy: 0.960099995136261\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128  \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved  for Validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 250,378\n",
      "Trainable params: 250,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 8s 176us/step - loss: 2.2935 - accuracy: 0.1340 - val_loss: 2.2815 - val_accuracy: 0.2072\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 2.2197 - accuracy: 0.2819 - val_loss: 2.0127 - val_accuracy: 0.3854\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 8s 172us/step - loss: 1.2767 - accuracy: 0.5595 - val_loss: 0.8060 - val_accuracy: 0.7271\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 8s 170us/step - loss: 0.6086 - accuracy: 0.8047 - val_loss: 0.5993 - val_accuracy: 0.7884\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 8s 164us/step - loss: 0.4118 - accuracy: 0.8758 - val_loss: 0.3251 - val_accuracy: 0.9024\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 8s 169us/step - loss: 0.3185 - accuracy: 0.9050 - val_loss: 0.2727 - val_accuracy: 0.9185\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 8s 167us/step - loss: 0.2695 - accuracy: 0.9199 - val_loss: 0.2356 - val_accuracy: 0.9312\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 8s 171us/step - loss: 0.2276 - accuracy: 0.9327 - val_loss: 0.2026 - val_accuracy: 0.9389\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 8s 174us/step - loss: 0.1987 - accuracy: 0.9411 - val_loss: 0.1857 - val_accuracy: 0.9437\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 9s 184us/step - loss: 0.1786 - accuracy: 0.9480 - val_loss: 0.1792 - val_accuracy: 0.9466\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.1611 - accuracy: 0.9520 - val_loss: 0.1640 - val_accuracy: 0.9507\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 9s 178us/step - loss: 0.1494 - accuracy: 0.9558 - val_loss: 0.1544 - val_accuracy: 0.9553\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 8s 176us/step - loss: 0.1360 - accuracy: 0.9591 - val_loss: 0.1448 - val_accuracy: 0.9569\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.1259 - accuracy: 0.9629 - val_loss: 0.1586 - val_accuracy: 0.9531\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.1173 - accuracy: 0.9653 - val_loss: 0.1453 - val_accuracy: 0.9573\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.1086 - accuracy: 0.9678 - val_loss: 0.1411 - val_accuracy: 0.9579\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 6s 131us/step - loss: 0.1008 - accuracy: 0.9705 - val_loss: 0.1433 - val_accuracy: 0.9577\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 7s 143us/step - loss: 0.0949 - accuracy: 0.9725 - val_loss: 0.1321 - val_accuracy: 0.9603\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 7s 138us/step - loss: 0.0859 - accuracy: 0.9748 - val_loss: 0.1243 - val_accuracy: 0.9638\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 7s 141us/step - loss: 0.0818 - accuracy: 0.9762 - val_loss: 0.1286 - val_accuracy: 0.9613\n",
      "10000/10000 [==============================] - 1s 56us/step\n",
      "Test score: 0.1285100500493776\n",
      "Test accuracy: 0.9620000123977661\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility\n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128  \n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved  for Validation\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784 \n",
    "RESHAPED = 784 \n",
    "# \n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, I chose to increase the number of hidden layers from 2, to 3, to 5 and then finally to 10. \n",
    "For the experiment with 2 hidden layers, I had a total params at 118,282, I had 94.56% on the training accuracy, 94.98% on validation and 94.63% on the test accuracy.\n",
    "\n",
    "With 3 hidden layers, I had a total params at 134,794, I had 95.66% on the training accuracy, 95.74% on validation and 95.47% on the test accuracy.\n",
    "\n",
    "With 5 hidden layers, I had a total params at 167,818, I had 96.59% on the training accuracy, 95.97% on validation and 96.01% on the test accuracy.\n",
    "\n",
    "With 10 hidden layers, I had a total params at 250,378, I had 97.62% on the training accuracy, 96.13% on validation and 96.20% on the test accuracy.\n",
    "\n",
    "As you can see with the increasing number of hidden layers it significantly increases our number of params as well as each of the percentages. This is important because we are looking for digits to be recognized. Adding in hidden layers improves the accuracy all the way around. This way more handwritten digits can be recognized with additional hidden layers. I noticed that when I only added in 2 hidden layers, that the training accuracy was lower then the test accuracy. This shouldn't happen and tells us that some changes need to be made to keep the training accuracy higher than the test accuracy. So if we looked at the experiement with 3 hidden layers, we can see that now the training accuracy is at 95.66% and the test accuracy is now lower at 95.47%. The same thing exists when we add in 5 and 10 hidden layers. When we add in 10 hidden layters while our params are greatly increased from 5 params, the percentages didn't chage as drastically as I thought they would. I suspect that once you add in so many layers,eventually it will not improve the accuracy anymore, and other paramerters will need to be adjusted to achive a higher accuracy. In addition, it also may be thoughtful to add in an occasional dropout to improve accuracy futher. This assignment did not ask for us to do that, so I didn't test that theory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
